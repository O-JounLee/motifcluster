---
title: "Using the `motifclustr` package"
author: "William George Underwood"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Using the `motifclustr` package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
header-includes:
  - \usepackage{tikz}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#",
  set.seed(200401293)
)
```

\tableofcontents

# Introduction

TODO consistent variable names
TODO name args

This vignette demonstrates how to use the `motifclustr` package for motif-based
spectral clustering of weighted directed networks.
These methods are detailed in the paper
*Motif-Based Spectral Clustering of Weighted Directed Networks*,
available on the [arXiv](arxivpaper).
TODO format arxiv link
The functionality of the `motifclustr` package falls into two main categories:

- Sampling random weighted directed networks
- Building motif adjacency matrices (MAMs)
- Spectral clustering with MAMs

This vignette comprehensibly demonstrates all of these functionalities,
showcasing the full capability of the `motifclustr` package.
The package is loaded with

```{r}
library(motifclustr)
```

# Building motif adjacency matrices

The main novelty in the `motifclustr` package is its ability to
build a wide variety of MAMs, and to do so quickly.
There are several options to consider when building an MAM,
and these options are covered in this section.

## An example network

In order to demonstrate the construction of MAMs,
we first need a small weighted directed network $\mathcal{G}$ to use as an example.
Note that throughout this package we represent networks by their
weighted directed adjacency matrices (possibly in sparse form).
This means that for use alongside packages such as `igraph`,
one must manually convert between adjacency matrices and
`igraph` objects.

```{r}
toy_adj_mat <- matrix(c(
  0, 2, 0, 0,
  0, 0, 2, 3,
  0, 4, 0, 0,
  4, 0, 5, 0
), byrow = TRUE, nrow = 4)
```

TODO plot network
\begin{tikzpicture}
\end{tikzpicture}

## Basic motif adjacency matrix construction

The `build_motif_adjacency_matrix` function is the main workhorse
for building MAMs with `motifclustr`.
Let's use it to build an MAM for our example network $\mathcal{G}$.
First we must choose a motif to look for;
a full list can be obtained with:

```{r}
get_motif_names()
```

Let's use the
3-cycle motif $\mathcal{M}_1$.

```{r}
mam <- build_motif_adjacency_matrix(toy_adj_mat, "M1")
prmatrix(mam)
```

Note that all the entries are zero except for entries
$(1,2)$,
$(1,4)$,
$(4,1)$,
and
$(4,2)$.
This is because vertices 1, 2 and 4 form an exact copy of
the motif $\mathcal{M}_1$ in the network $\mathcal{G}$,
and the  $(i,j)$th MAM entry simply counts the number of instances
containing both vertices $i$ and $j$.

## Functional and structural motif adjacency matrices

Looking at our example network $\mathcal{G}$ again,
you might notice that there is seemingly another instance
of the motif $\mathcal{M}_1$
in our network $\mathcal{G}$,
on the vertices 2, 3 and 4,
albeit with an extra edge from 2 to 3.
The reason for this is that we instructed
`build_motif_adjacency_matrix`
to look for *structural* motif instances (this is the default).
Structural instances require an exact match, with no extra edges.
If we want to also include instances which may have extra edges present,
we can instead use functional motif instances:

```{r}
mam <- build_motif_adjacency_matrix(toy_adj_mat, "M1", motif_type = "func")
prmatrix(mam)
```

This time we also pick up the 3-cycle on vertices 2, 3 and 4.
Vertices 2 and 4 therefore occur in two distinct instances
of the motif, and so their motif adjacency matrix entries are equal
to two.

## Weighted motif adjacency matrices

Our example network has weighted edges,
which we have not yet taken into account:
so far our MAMs have been simply counting
instances of motifs.
This is because the default weighting scheme is
"unweighted", assigning every instance a weight of one.

### Mean-weighted instances

We can instead use the "mean" weighting scheme,
where every instance is assigned a weight equal to its
mean edge weight.
The $(i,j)$th MAM entry is then defined as the sum of these
instance weights across all instances containing
both vertices $i$ and $j$:

```{r}
mam <- build_motif_adjacency_matrix(toy_adj_mat, "M1", motif_type = "func",
  weight_type = "mean")
prmatrix(mam)
```

The 3-cycle on vertices 1, 2 and 4
has edge weights of 2, 3 and 4, so its mean edge weight is 3.
Similarly the 3-cycle on vertices 2, 3 and 4 has mean edge weight of 4.
Vertices 2 and 4 appear in both, so their shared MAM entries are the
sum of these two mean weights, which is 7.

### Product-weighted instances

We can also use the "product" weighting scheme,
where every instance is assigned a weight equal to the
product of its edge weights.
The $(i,j)$th MAM entry is then defined as the sum of these
instance weights across all instances containing
both vertices $i$ and $j$:

```{r}
mam <- build_motif_adjacency_matrix(toy_adj_mat, "M1", motif_type = "func",
  weight_type = "product")
prmatrix(mam)
```

The 3-cycle on vertices 1, 2 and 4
has edge weights of 2, 3 and 4, so the product of its edge weights is 24.
Similarly the 3-cycle on vertices 2, 3 and 4 has mean edge weight of 60.
Vertices 2 and 4 appear in both, so their shared MAM entries are the
sum of these two product weights, which is 84.

### Computation method

The final argument to `build_motif_adjacency_matrix`
is the "method" argument.
This does not affect the value returned but may impact the
amount of time taken to return the MAM.
In general the "sparse" method (the default)
is faster on large sparse networks.
The "dense" method, which uses fewer operations but on
denser matrices, tends to be faster for small dense networks.


```{r}
mam_1 <- build_motif_adjacency_matrix(toy_adj_mat, "M1", method = "sparse")
mam_2 <- build_motif_adjacency_matrix(toy_adj_mat, "M1", method = "dense")
print(all(mam_1 == mam_2))
```

# Sampling random weighted directed networks

Building adjacency matrices by hand is tedious,
so it is useful to have methods for generating
the adjacency matrices of networks drawn from some probabilistic model.
We use (weighted) directed stochastic block models (DSBMs) and
(weighted) bipartite stochastic block models (BSBMs).

## Directed stochastic block models

First let's sample a the adjacency matrix of a DSBM
which has two blocks of vertices;
the first containing 5 vertices and the second containing 3.
We use strong within-block connections,
with the diagonal entries of the connection matrix set to 0.9.
The between-block connections are much weaker, with the off-diagonal
connection matrix entries set to 0.1.
Note how the resulting adjacency matrix is dense
on its diagonal blocks
$\{1, \dots, 5\} \times \{1, \dots, 5\}$
and
$\{6, \dots, 8\} \times \{6, \dots, 8\}$,
and is sparse on its off-diagonal blocks
$\{1, \dots, 5\} \times \{6, \dots, 8\}$
and
$\{6, \dots, 8\} \times \{1, \dots, 5\}$.
All the entries which lie exactly on the diagonal will
always be zero,
since we consider networks without self-loops.

```{r}
block_sizes = c(5, 3)
connection_matrix = matrix(c(
  0.9, 0.2,
  0.2, 0.9
), nrow = 2, byrow = TRUE)
adj_mat = sample_dsbm(block_sizes, connection_matrix)
prmatrix(adj_mat)
```

### Constant-weighted directed stochastic block models

The adjacency matrix above has binary entries,
indicating that it is the adjacency matrix of an unweighted
directed network.
The `motifclustr` package also allows sampling of weighted directed networks.
The simplest example of this is "constant" weighting,
where we simply multiply each block of the adjacency matrix by a constant.

```{r}
weight_matrix = matrix(c(
  5, 2,
  2, 5
), nrow = 2, byrow = TRUE)
adj_mat = sample_dsbm(block_sizes, connection_matrix, weight_matrix,
                      weight_type = "constant")
prmatrix(adj_mat)
```

### Poisson-weighted directed stochastic block models

We can also use weights drawn randomly from a Poisson distribution,
where each block in the adjacency matrix has its own mean parameter.
This returns an adjacency matrix with weights which could be any natural
number, but is equal in expectation to the constant version.
Note that in this scheme it is possible for the weight to be zero,
removing an edge which might have otherwise been present.

```{r}
adj_mat = sample_dsbm(block_sizes, connection_matrix, weight_matrix,
                      weight_type = "poisson")
prmatrix(adj_mat)
```

# Spectral embedding with motif adjacency matrices

Spectral methods involve performing eigenvalue and
eigenvector operations on matrices related to networks.
We work here with weighted *undirected* networks
(which have symmetric adjacency matrices),
because motif adjacency matrices are always symmetric.

## Laplacian matrices

We can construct two types of Laplacian matrix for a network
using the `motifclustr` package.
First we create a toy example of a weighted undirected adjacency matrix.

```{r}
toy_adj_mat_sym <- matrix(c(
  0, 2, 0, 0,
  2, 0, 4, 3,
  0, 4, 0, 5,
  0, 3, 5, 0
), byrow = TRUE, nrow = 4)
```

### Combinatorial Laplacian

The combinatorial Laplacian of an adjacency matrix $G$ is
$L_\mathrm{c} = D - G$,
where $D$ is the diagonal matrix of weighted vertex degrees:

```{r}
Lc = build_laplacian(toy_adj_mat_sym, "comb")
prmatrix(Lc)
```

### Random-walk Laplacian

The random-walk Laplacian of an adjacency matrix $G$ is
$L_\mathrm{c} = I - D^{-1}G$,
where $D$ is the diagonal matrix of weighted vertex degrees:

```{r}
Lrw = build_laplacian(toy_adj_mat_sym, "rw")
prmatrix(round(Lrw, 2))
```

## Laplace embedding

Once we have constructed the desired Laplacian matrix,
we use it to embed each vertex into $\mathbb{R}^l$
by finding the eigenvectors associated with
its first (smallest magnitude) eigenvalues.
Below we use the random-walk Laplacian,
and embedding dimension $l=2$:

```{r}
spectrum = run_laplace_embedding(toy_adj_mat_sym, 2, "rw")
round(spectrum$vects, 2)
round(spectrum$vals, 4)
```

For a random-walk Laplacian,
the first eigenvalue is always zero (up to machine precision)
and its corresponding eigenvector is constant.

## Motif embedding

Motif embedding is simply the process of building a MAM
and then performing Laplace embedding with it.
This function also makes the MAM available.
As an example we go back to the first toy network,
which is weighted and directed.

```{r}
spectrum = run_motif_embedding(toy_adj_mat, "M1", "func", "unweighted", "sparse", 2, "rw")
round(spectrum$vals, 4)
round(spectrum$vects, 2)
prmatrix(spectrum$motif_adj_mat)
```

An artefact of building MAMs is that although the original network may
be connected, there is no guarantee that the MAM is also connected.
Hence the MAM is restricted to its largest connected component
before the Laplacian is formed.
This is demonstrated below,
where only three of the four vertices are embedded.

```{r}
toy_adj_mat <- matrix(c(
  0, 0, 0, 0,
  0, 0, 2, 3,
  0, 4, 0, 0,
  4, 0, 5, 0
), byrow = TRUE, nrow = 4)
spectrum = run_motif_embedding(toy_adj_mat, "M1", "func", "unweighted", "sparse", 2, "rw")
round(spectrum$vals, 4)
round(spectrum$vects, 2)
```

# Motif-based spectral clustering

The overall aim is to use motifs for spectral clustering,
so now we see how to extract clusters from the motif-based
eigenvector embeddings.

TODO write this after writing the code for it










[arxivpaper]: https://arxiv.org/abs/2004.01293
